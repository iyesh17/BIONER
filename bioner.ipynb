{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8572400,"sourceType":"datasetVersion","datasetId":5125905},{"sourceId":8572682,"sourceType":"datasetVersion","datasetId":5126075},{"sourceId":8576239,"sourceType":"datasetVersion","datasetId":5128417},{"sourceId":8670719,"sourceType":"datasetVersion","datasetId":5196371}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nimport torch.nn as nn\nimport torch.optim as optim\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom tqdm import tqdm\nfrom sklearn.metrics import classification_report\n\ntrain_chem_path = r'/kaggle/input/bc5cdr-chem/train.tsv'\ntest_chem_path = r'/kaggle/input/bc5cdr-chem/test.tsv'\nval_chem_path = r'/kaggle/input/bc5cdr-chem/devel.tsv'\n\ntrain_disease_path = r'/kaggle/input/ncbi-disease/train.tsv'\ntest_disease_path = r'/kaggle/input/ncbi-disease/test.tsv'\nval_disease_path = r'/kaggle/input/ncbi-disease/devel.tsv'\n\ndef load_data(file_path):\n    try:\n        data = pd.read_csv(file_path, delimiter='\\t', header=None, names=[\"tokens\", \"labels\"])\n        print(f\"Loaded data from {file_path} with shape {data.shape}\")\n        return data\n    except Exception as e:\n        print(f\"Error loading data from {file_path}: {e}\")\n        raise\n\ndef tokenize_and_preserve_labels(sentence, text_labels, tokenizer, label_to_id, max_len):\n    tokenized_sentence = []\n    labels = []\n\n    for word, label in zip(sentence, text_labels):\n        if not isinstance(word, str) or not isinstance(label, str):\n            continue  # Skip non-string entries\n\n        tokenized_word = tokenizer.tokenize(word)\n        tokenized_sentence.extend(tokenized_word)\n        labels.extend([label] + [-100] * (len(tokenized_word) - 1))\n\n    if len(tokenized_sentence) > max_len - 2:\n        tokenized_sentence = tokenized_sentence[:max_len - 2]\n        labels = labels[:max_len - 2]\n\n    tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"]\n    labels = [-100] + labels + [-100]\n    input_ids = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n    attention_mask = [1] * len(input_ids)\n    label_ids = [label_to_id.get(label, -100) for label in labels]\n\n    while len(input_ids) < max_len:\n        input_ids.append(0)\n        attention_mask.append(0)\n        label_ids.append(-100)\n\n    return input_ids, label_ids, attention_mask\n\ndef process_dataset_with_special_tokens(dataset_path, tokenizer, label_to_id, max_length=128):\n    data = load_data(dataset_path)\n    input_ids = []\n    tag_ids = []\n    attention_masks = []\n\n    grouped_data = data.groupby((data['tokens'] == '.').cumsum()).apply(lambda x: (x['tokens'].tolist(), x['labels'].tolist()))\n    for sentence, labels in grouped_data:\n        try:\n            ids, label_ids, mask = tokenize_and_preserve_labels(sentence, labels, tokenizer, label_to_id, max_length)\n            input_ids.append(ids)\n            tag_ids.append(label_ids)\n            attention_masks.append(mask)\n        except Exception as e:\n            print(f\"Error processing sentence: {sentence}\")\n            print(f\"Labels: {labels}\")\n            print(f\"Exception: {e}\")\n            continue  # Skip problematic sentences\n\n    return input_ids, tag_ids, attention_masks\n\nclass NERDataset(Dataset):\n    def __init__(self, input_ids, tag_ids, attention_masks):\n        self.input_ids = input_ids\n        self.tag_ids = tag_ids\n        self.attention_masks = attention_masks\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n            'tag_ids': torch.tensor(self.tag_ids[idx], dtype=torch.long),\n            'attention_mask': torch.tensor(self.attention_masks[idx], dtype=torch.long)\n        }\n\nclass NERModel(nn.Module):\n    def __init__(self, num_tags):\n        super(NERModel, self).__init__()\n        self.bert = AutoModelForTokenClassification.from_pretrained(\"allenai/scibert_scivocab_uncased\", num_labels=num_tags)\n\n    def forward(self, input_ids, attention_mask, tags=None):\n        outputs = self.bert(input_ids, attention_mask=attention_mask, labels=tags)\n        return outputs.loss if tags is not None else outputs.logits\n\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n\nlabel_to_id = {'B': 0, 'I': 1, 'O': 2}","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-01T16:36:45.964344Z","iopub.execute_input":"2024-10-01T16:36:45.964756Z","iopub.status.idle":"2024-10-01T16:36:54.318594Z","shell.execute_reply.started":"2024-10-01T16:36:45.964730Z","shell.execute_reply":"2024-10-01T16:36:54.317822Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eaa8fea73624c6ab68d7791c4a39bce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a1439c3c9dd4063bcf36038ca439938"}},"metadata":{}}]},{"cell_type":"code","source":"# Process train dataset for chemicals\ntrain_chem_input_ids, train_chem_tag_ids, train_chem_attention_masks = process_dataset_with_special_tokens(train_chem_path, tokenizer, label_to_id, max_length=128)\ntrain_chem_dataset = NERDataset(train_chem_input_ids, train_chem_tag_ids, train_chem_attention_masks)\ntrain_chem_dataloader = DataLoader(train_chem_dataset, batch_size=4, shuffle=True)\n\n# Process train dataset for NCBI diseases\ntrain_disease_input_ids, train_disease_tag_ids, train_disease_attention_masks = process_dataset_with_special_tokens(train_disease_path, tokenizer, label_to_id, max_length=128)\ntrain_disease_dataset = NERDataset(train_disease_input_ids, train_disease_tag_ids, train_disease_attention_masks)\ntrain_disease_dataloader = DataLoader(train_disease_dataset, batch_size=4, shuffle=True)\n\n# Combine chemical and NCBI disease datasets for training\ncombined_train_dataset = ConcatDataset([train_chem_dataset, train_disease_dataset])\ncombined_train_dataloader = DataLoader(combined_train_dataset, batch_size=4, shuffle=True)\n\n# Process test dataset for chemicals\ntest_chem_input_ids, test_chem_tag_ids, test_chem_attention_masks = process_dataset_with_special_tokens(test_chem_path, tokenizer, label_to_id, max_length=128)\ntest_chem_dataset = NERDataset(test_chem_input_ids, test_chem_tag_ids, test_chem_attention_masks)\ntest_chem_dataloader = DataLoader(test_chem_dataset, batch_size=4, shuffle=False)\n\n# Process test dataset for NCBI diseases\ntest_disease_input_ids, test_disease_tag_ids, test_disease_attention_masks = process_dataset_with_special_tokens(test_disease_path, tokenizer, label_to_id, max_length=128)\ntest_disease_dataset = NERDataset(test_disease_input_ids, test_disease_tag_ids, test_disease_attention_masks)\ntest_disease_dataloader = DataLoader(test_disease_dataset, batch_size=4, shuffle=False)\n\n# Process validation dataset for chemicals\nval_chem_input_ids, val_chem_tag_ids, val_chem_attention_masks = process_dataset_with_special_tokens(val_chem_path, tokenizer, label_to_id, max_length=128)\nval_chem_dataset = NERDataset(val_chem_input_ids, val_chem_tag_ids, val_chem_attention_masks)\nval_chem_dataloader = DataLoader(val_chem_dataset, batch_size=4, shuffle=False)\n\n# Process validation dataset for NCBI diseases\nval_disease_input_ids, val_disease_tag_ids, val_disease_attention_masks = process_dataset_with_special_tokens(val_disease_path, tokenizer, label_to_id, max_length=128)\nval_disease_dataset = NERDataset(val_disease_input_ids, val_disease_tag_ids, val_disease_attention_masks)\nval_disease_dataloader = DataLoader(val_disease_dataset, batch_size=4, shuffle=False)\n\n# Initialize the model and optimizer\nmodel = NERModel(num_tags=len(label_to_id))\noptimizer = optim.Adam(model.parameters(), lr=5e-5)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T16:36:54.320485Z","iopub.execute_input":"2024-10-01T16:36:54.321141Z","iopub.status.idle":"2024-10-01T16:37:32.859738Z","shell.execute_reply.started":"2024-10-01T16:36:54.321107Z","shell.execute_reply":"2024-10-01T16:37:32.858812Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Loaded data from /kaggle/input/bc5cdr-chem/train.tsv with shape (122729, 2)\nLoaded data from /kaggle/input/ncbi-disease/train.tsv with shape (135615, 2)\nLoaded data from /kaggle/input/bc5cdr-chem/test.tsv with shape (124676, 2)\nLoaded data from /kaggle/input/ncbi-disease/test.tsv with shape (24488, 2)\nLoaded data from /kaggle/input/bc5cdr-chem/devel.tsv with shape (117391, 2)\nLoaded data from /kaggle/input/ncbi-disease/devel.tsv with shape (23959, 2)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"095af4271267476e8f3f97dc2c393289"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_model(model, train_dataloader, optimizer, num_epochs=3, device='cpu'):\n    model.train()\n    model.to(device)\n    for epoch in range(num_epochs):\n        total_loss = 0\n        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n        for batch in progress_bar:\n            optimizer.zero_grad()\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            tags = batch['tag_ids'].long().to(device)\n            loss = model(input_ids, attention_mask, tags)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            progress_bar.set_postfix(loss=total_loss/len(train_dataloader))\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_dataloader)}\")\ndef evaluate_model(model, dataloader, device='cpu'):\n    model.eval()\n    model.to(device)\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            tags = batch['tag_ids'].long().to(device)\n\n            logits = model(input_ids, attention_mask)\n            loss_fct = nn.CrossEntropyLoss()\n            active_loss = attention_mask.view(-1) == 1\n            active_logits = logits.view(-1, logits.shape[-1])\n            active_labels = torch.where(\n                active_loss, tags.view(-1), torch.tensor(loss_fct.ignore_index).type_as(tags)\n            )\n            loss = loss_fct(active_logits, active_labels)\n\n            total_loss += loss.item()\n\n            preds = torch.argmax(logits, dim=2).cpu().numpy()\n            labels = tags.cpu().numpy()\n\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n\n    avg_loss = total_loss / len(dataloader)\n    return avg_loss, all_preds, all_labels","metadata":{"execution":{"iopub.status.busy":"2024-10-01T16:37:32.861129Z","iopub.execute_input":"2024-10-01T16:37:32.861601Z","iopub.status.idle":"2024-10-01T16:37:32.873830Z","shell.execute_reply.started":"2024-10-01T16:37:32.861568Z","shell.execute_reply":"2024-10-01T16:37:32.872839Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrain_model(model, combined_train_dataloader, optimizer, num_epochs=3, device=device)\n\ntest_loss, test_preds, test_labels = evaluate_model(model, test_chem_dataloader, device=device)\nval_loss, val_preds, val_labels = evaluate_model(model, val_chem_dataloader, device=device)\n\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Validation Loss: {val_loss}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-01T16:37:32.876530Z","iopub.execute_input":"2024-10-01T16:37:32.876910Z","iopub.status.idle":"2024-10-01T16:56:19.638180Z","shell.execute_reply.started":"2024-10-01T16:37:32.876879Z","shell.execute_reply":"2024-10-01T16:56:19.636712Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Epoch 1/3: 100%|██████████| 3025/3025 [05:36<00:00,  8.99it/s, loss=0.0697]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Loss: 0.06967389219148279\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/3:  30%|██▉       | 899/3025 [01:42<04:00,  8.84it/s, loss=0.00984] IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nEpoch 3/3: 100%|██████████| 3025/3025 [05:44<00:00,  8.79it/s, loss=0.0275]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/3, Loss: 0.027530311551511488\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 1623/1623 [00:53<00:00, 30.42it/s]\nEvaluating: 100%|██████████| 1480/1480 [00:48<00:00, 30.37it/s]","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.061801336536311306\nValidation Loss: 0.05586650152900138\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"def generate_classification_report(labels, preds, label_to_id):\n    id_to_label = {id: label for label, id in label_to_id.items()}\n    labels_flat = []\n    preds_flat = []\n\n    for label_seq, pred_seq in zip(labels, preds):\n        for label, pred in zip(label_seq, pred_seq):\n            if label != -100:  # Ignore padding tokens\n                labels_flat.append(id_to_label[label])\n                preds_flat.append(id_to_label[pred])\n\n    report = classification_report(labels_flat, preds_flat, digits=4)\n    return report\n\ntest_report = generate_classification_report(test_labels, test_preds, label_to_id)\nval_report = generate_classification_report(val_labels, val_preds, label_to_id)\n\nprint(\"Test Classification Report:\\n\", test_report)\nprint(\"Validation Classification Report:\\n\", val_report)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T16:56:19.639655Z","iopub.execute_input":"2024-10-01T16:56:19.640007Z","iopub.status.idle":"2024-10-01T16:56:22.591976Z","shell.execute_reply.started":"2024-10-01T16:56:19.639974Z","shell.execute_reply":"2024-10-01T16:56:22.591071Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Test Classification Report:\n               precision    recall  f1-score   support\n\n           B     0.8985    0.8806    0.8895      5378\n           I     0.7118    0.8145    0.7597      1628\n           O     0.9930    0.9919    0.9925    117654\n\n    accuracy                         0.9848    124660\n   macro avg     0.8678    0.8957    0.8805    124660\nweighted avg     0.9853    0.9848    0.9850    124660\n\nValidation Classification Report:\n               precision    recall  f1-score   support\n\n           B     0.9120    0.8978    0.9048      5342\n           I     0.7753    0.8251    0.7994      1744\n           O     0.9931    0.9928    0.9930    110128\n\n    accuracy                         0.9860    117214\n   macro avg     0.8935    0.9052    0.8991    117214\nweighted avg     0.9862    0.9860    0.9861    117214\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nsave_directory = \"saved_scibert_model\"\n\nif not os.path.exists(save_directory):\n    os.makedirs(save_directory)\n\nmodel_save_path = os.path.join(save_directory, \"pytorch_model.bin\")\ntorch.save(model.state_dict(), model_save_path)\n\ntokenizer_save_path = os.path.join(save_directory, \"tokenizer\")\ntokenizer.save_pretrained(tokenizer_save_path)\n\nprint(f\"Model saved to {model_save_path}\")\nprint(f\"Tokenizer saved to {tokenizer_save_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-01T16:56:22.593106Z","iopub.execute_input":"2024-10-01T16:56:22.593423Z","iopub.status.idle":"2024-10-01T16:56:23.163124Z","shell.execute_reply.started":"2024-10-01T16:56:22.593396Z","shell.execute_reply":"2024-10-01T16:56:23.162229Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Model saved to saved_scibert_model/pytorch_model.bin\nTokenizer saved to saved_scibert_model/tokenizer\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nimport torch.nn as nn\nimport torch.optim as optim\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom tqdm import tqdm\nfrom sklearn.metrics import classification_report\n\n# Define paths for the datasets\ntrain_chem_path = r'/kaggle/input/bc5cdr-chem/train.tsv'\ntest_chem_path = r'/kaggle/input/bc5cdr-chem/test.tsv'\nval_chem_path = r'/kaggle/input/bc5cdr-chem/devel.tsv'\n\ntrain_disease_path = r'/kaggle/input/ncbi-disease/train.tsv'\ntest_disease_path = r'/kaggle/input/ncbi-disease/test.tsv'\nval_disease_path = r'/kaggle/input/ncbi-disease/devel.tsv'\n\ndef load_data(file_path):\n    try:\n        data = pd.read_csv(file_path, delimiter='\\t', header=None, names=[\"tokens\", \"labels\"])\n        print(f\"Loaded data from {file_path} with shape {data.shape}\")\n        return data\n    except Exception as e:\n        print(f\"Error loading data from {file_path}: {e}\")\n        raise\n\ndef tokenize_and_preserve_labels(sentence, text_labels, tokenizer, label_to_id, max_len):\n    tokenized_sentence = []\n    labels = []\n\n    for word, label in zip(sentence, text_labels):\n        if not isinstance(word, str) or not isinstance(label, str):\n            continue  # Skip non-string entries\n\n        tokenized_word = tokenizer.tokenize(word)\n        tokenized_sentence.extend(tokenized_word)\n        labels.extend([label] + [-100] * (len(tokenized_word) - 1))\n\n    if len(tokenized_sentence) > max_len - 2:\n        tokenized_sentence = tokenized_sentence[:max_len - 2]\n        labels = labels[:max_len - 2]\n\n    tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"]\n    labels = [-100] + labels + [-100]\n    input_ids = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n    attention_mask = [1] * len(input_ids)\n    label_ids = [label_to_id.get(label, -100) for label in labels]\n\n    while len(input_ids) < max_len:\n        input_ids.append(0)\n        attention_mask.append(0)\n        label_ids.append(-100)\n\n    return input_ids, label_ids, attention_mask\n\ndef process_dataset_with_special_tokens(dataset_path, tokenizer, label_to_id, max_length=128):\n    data = load_data(dataset_path)\n    input_ids = []\n    tag_ids = []\n    attention_masks = []\n\n    grouped_data = data.groupby((data['tokens'] == '.').cumsum()).apply(lambda x: (x['tokens'].tolist(), x['labels'].tolist()))\n    for sentence, labels in grouped_data:\n        try:\n            ids, label_ids, mask = tokenize_and_preserve_labels(sentence, labels, tokenizer, label_to_id, max_length)\n            input_ids.append(ids)\n            tag_ids.append(label_ids)\n            attention_masks.append(mask)\n        except Exception as e:\n            print(f\"Error processing sentence: {sentence}\")\n            print(f\"Labels: {labels}\")\n            print(f\"Exception: {e}\")\n            continue  # Skip problematic sentences\n\n    return input_ids, tag_ids, attention_masks\n\nclass NERDataset(Dataset):\n    def __init__(self, input_ids, tag_ids, attention_masks):\n        self.input_ids = input_ids\n        self.tag_ids = tag_ids\n        self.attention_masks = attention_masks\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n            'tag_ids': torch.tensor(self.tag_ids[idx], dtype=torch.long),\n            'attention_mask': torch.tensor(self.attention_masks[idx], dtype=torch.long)\n        }\n\nclass NERModel(nn.Module):\n    def __init__(self, num_tags):\n        super(NERModel, self).__init__()\n        self.bert = AutoModelForTokenClassification.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\", num_labels=num_tags)\n\n    def forward(self, input_ids, attention_mask, tags=None):\n        outputs = self.bert(input_ids, attention_mask=attention_mask, labels=tags)\n        return outputs.loss if tags is not None else outputs.logits\n\ntokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n\nlabel_to_id = {'B': 0, 'I': 1, 'O': 2}","metadata":{"execution":{"iopub.status.busy":"2024-10-01T16:56:23.164581Z","iopub.execute_input":"2024-10-01T16:56:23.165063Z","iopub.status.idle":"2024-10-01T16:56:24.356607Z","shell.execute_reply.started":"2024-10-01T16:56:23.165028Z","shell.execute_reply":"2024-10-01T16:56:24.355571Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/313 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bde1eec081f744ffb114d2e0a3fcdfe2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3b17145cd724e7b8ea215bd573b393f"}},"metadata":{}}]},{"cell_type":"code","source":"# Process train dataset for chemicals\ntrain_chem_input_ids, train_chem_tag_ids, train_chem_attention_masks = process_dataset_with_special_tokens(train_chem_path, tokenizer, label_to_id, max_length=128)\ntrain_chem_dataset = NERDataset(train_chem_input_ids, train_chem_tag_ids, train_chem_attention_masks)\ntrain_chem_dataloader = DataLoader(train_chem_dataset, batch_size=4, shuffle=True)\n\n# Process train dataset for NCBI diseases\ntrain_disease_input_ids, train_disease_tag_ids, train_disease_attention_masks = process_dataset_with_special_tokens(train_disease_path, tokenizer, label_to_id, max_length=128)\ntrain_disease_dataset = NERDataset(train_disease_input_ids, train_disease_tag_ids, train_disease_attention_masks)\ntrain_disease_dataloader = DataLoader(train_disease_dataset, batch_size=4, shuffle=True)\n\n# Combine chemical and NCBI disease datasets for training\ncombined_train_dataset = ConcatDataset([train_chem_dataset, train_disease_dataset])\ncombined_train_dataloader = DataLoader(combined_train_dataset, batch_size=4, shuffle=True)\n\n# Process test dataset for chemicals\ntest_chem_input_ids, test_chem_tag_ids, test_chem_attention_masks = process_dataset_with_special_tokens(test_chem_path, tokenizer, label_to_id, max_length=128)\ntest_chem_dataset = NERDataset(test_chem_input_ids, test_chem_tag_ids, test_chem_attention_masks)\ntest_chem_dataloader = DataLoader(test_chem_dataset, batch_size=4, shuffle=False)\n\n# Process test dataset for NCBI diseases\ntest_disease_input_ids, test_disease_tag_ids, test_disease_attention_masks = process_dataset_with_special_tokens(test_disease_path, tokenizer, label_to_id, max_length=128)\ntest_disease_dataset = NERDataset(test_disease_input_ids, test_disease_tag_ids, test_disease_attention_masks)\ntest_disease_dataloader = DataLoader(test_disease_dataset, batch_size=4, shuffle=False)\n\n# Process validation dataset for chemicals\nval_chem_input_ids, val_chem_tag_ids, val_chem_attention_masks = process_dataset_with_special_tokens(val_chem_path, tokenizer, label_to_id, max_length=128)\nval_chem_dataset = NERDataset(val_chem_input_ids, val_chem_tag_ids, val_chem_attention_masks)\nval_chem_dataloader = DataLoader(val_chem_dataset, batch_size=4, shuffle=False)\n\n# Process validation dataset for NCBI diseases\nval_disease_input_ids, val_disease_tag_ids, val_disease_attention_masks = process_dataset_with_special_tokens(val_disease_path, tokenizer, label_to_id, max_length=128)\nval_disease_dataset = NERDataset(val_disease_input_ids, val_disease_tag_ids, val_disease_attention_masks)\nval_disease_dataloader = DataLoader(val_disease_dataset, batch_size=4, shuffle=False)\n\n# Initialize the model and optimizer\nmodel = NERModel(num_tags=len(label_to_id))\noptimizer = optim.Adam(model.parameters(), lr=5e-5)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T16:56:24.357777Z","iopub.execute_input":"2024-10-01T16:56:24.358186Z","iopub.status.idle":"2024-10-01T16:57:01.208108Z","shell.execute_reply.started":"2024-10-01T16:56:24.358136Z","shell.execute_reply":"2024-10-01T16:57:01.207203Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Loaded data from /kaggle/input/bc5cdr-chem/train.tsv with shape (122729, 2)\nLoaded data from /kaggle/input/ncbi-disease/train.tsv with shape (135615, 2)\nLoaded data from /kaggle/input/bc5cdr-chem/test.tsv with shape (124676, 2)\nLoaded data from /kaggle/input/ncbi-disease/test.tsv with shape (24488, 2)\nLoaded data from /kaggle/input/bc5cdr-chem/devel.tsv with shape (117391, 2)\nLoaded data from /kaggle/input/ncbi-disease/devel.tsv with shape (23959, 2)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26d135a5a1164154a7680a02034db235"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_model(model, train_dataloader, optimizer, num_epochs=3, device='cpu'):\n    model.train()\n    model.to(device)\n    for epoch in range(num_epochs):\n        total_loss = 0\n        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n        for batch in progress_bar:\n            optimizer.zero_grad()\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            tags = batch['tag_ids'].long().to(device)\n            loss = model(input_ids, attention_mask, tags)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            progress_bar.set_postfix(loss=total_loss/len(train_dataloader))\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_dataloader)}\")\ndef evaluate_model(model, dataloader, device='cpu'):\n    model.eval()\n    model.to(device)\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            tags = batch['tag_ids'].long().to(device)\n\n            logits = model(input_ids, attention_mask)\n            loss_fct = nn.CrossEntropyLoss()\n            active_loss = attention_mask.view(-1) == 1\n            active_logits = logits.view(-1, logits.shape[-1])\n            active_labels = torch.where(\n                active_loss, tags.view(-1), torch.tensor(loss_fct.ignore_index).type_as(tags)\n            )\n            loss = loss_fct(active_logits, active_labels)\n\n            total_loss += loss.item()\n\n            preds = torch.argmax(logits, dim=2).cpu().numpy()\n            labels = tags.cpu().numpy()\n\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n\n    avg_loss = total_loss / len(dataloader)\n    return avg_loss, all_preds, all_labels","metadata":{"execution":{"iopub.status.busy":"2024-10-01T16:57:01.209588Z","iopub.execute_input":"2024-10-01T16:57:01.209877Z","iopub.status.idle":"2024-10-01T16:57:01.222063Z","shell.execute_reply.started":"2024-10-01T16:57:01.209852Z","shell.execute_reply":"2024-10-01T16:57:01.221194Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrain_model(model, combined_train_dataloader, optimizer, num_epochs=3, device=device)\n\ntest_loss, test_preds, test_labels = evaluate_model(model, test_chem_dataloader, device=device)\nval_loss, val_preds, val_labels = evaluate_model(model, val_chem_dataloader, device=device)\n\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Validation Loss: {val_loss}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-01T16:57:01.225289Z","iopub.execute_input":"2024-10-01T16:57:01.225552Z","iopub.status.idle":"2024-10-01T17:15:53.687894Z","shell.execute_reply.started":"2024-10-01T16:57:01.225530Z","shell.execute_reply":"2024-10-01T17:15:53.686991Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Epoch 1/3: 100%|██████████| 3025/3025 [05:43<00:00,  8.81it/s, loss=0.0646]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Loss: 0.06458271463942321\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/3: 100%|██████████| 3025/3025 [05:43<00:00,  8.81it/s, loss=0.0292] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/3, Loss: 0.029223973463361117\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/3: 100%|██████████| 3025/3025 [05:43<00:00,  8.81it/s, loss=0.0203] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/3, Loss: 0.020258636973925274\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 1623/1623 [00:53<00:00, 30.30it/s]\nEvaluating: 100%|██████████| 1480/1480 [00:48<00:00, 30.31it/s]","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.04753487426515764\nValidation Loss: 0.0516041841521975\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"def generate_classification_report(labels, preds, label_to_id):\n    id_to_label = {id: label for label, id in label_to_id.items()}\n    labels_flat = []\n    preds_flat = []\n\n    for label_seq, pred_seq in zip(labels, preds):\n        for label, pred in zip(label_seq, pred_seq):\n            if label != -100:  # Ignore padding tokens\n                labels_flat.append(id_to_label[label])\n                preds_flat.append(id_to_label[pred])\n\n    report = classification_report(labels_flat, preds_flat, digits=4)\n    return report\n\ntest_report = generate_classification_report(test_labels, test_preds, label_to_id)\nval_report = generate_classification_report(val_labels, val_preds, label_to_id)\n\nprint(\"Test Classification Report:\\n\", test_report)\nprint(\"Validation Classification Report:\\n\", val_report)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:15:53.689239Z","iopub.execute_input":"2024-10-01T17:15:53.689593Z","iopub.status.idle":"2024-10-01T17:15:56.606233Z","shell.execute_reply.started":"2024-10-01T17:15:53.689559Z","shell.execute_reply":"2024-10-01T17:15:56.605369Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Test Classification Report:\n               precision    recall  f1-score   support\n\n           B     0.8878    0.9248    0.9060      5375\n           I     0.7226    0.8704    0.7896      1628\n           O     0.9956    0.9909    0.9932    117601\n\n    accuracy                         0.9865    124604\n   macro avg     0.8687    0.9287    0.8963    124604\nweighted avg     0.9874    0.9865    0.9868    124604\n\nValidation Classification Report:\n               precision    recall  f1-score   support\n\n           B     0.8954    0.9251    0.9100      5341\n           I     0.7529    0.8576    0.8018      1734\n           O     0.9947    0.9909    0.9928    110050\n\n    accuracy                         0.9859    117125\n   macro avg     0.8810    0.9245    0.9015    117125\nweighted avg     0.9866    0.9859    0.9862    117125\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n# Directory to save the model and tokenizer\nsave_directory = \"saved_biobert_model\"\n\nif not os.path.exists(save_directory):\n    os.makedirs(save_directory)\n\n# Save the model\nmodel_save_path = os.path.join(save_directory, \"pytorch_model.bin\")\ntorch.save(model.state_dict(), model_save_path)\n\n# Save the tokenizer\ntokenizer_save_path = os.path.join(save_directory, \"tokenizer\")\ntokenizer.save_pretrained(tokenizer_save_path)\n\nprint(f\"Model saved to {model_save_path}\")\nprint(f\"Tokenizer saved to {tokenizer_save_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:15:56.607505Z","iopub.execute_input":"2024-10-01T17:15:56.607848Z","iopub.status.idle":"2024-10-01T17:15:57.165090Z","shell.execute_reply.started":"2024-10-01T17:15:56.607815Z","shell.execute_reply":"2024-10-01T17:15:57.164203Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Model saved to saved_biobert_model/pytorch_model.bin\nTokenizer saved to saved_biobert_model/tokenizer\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForTokenClassification\n\n# Load the tokenizer\nbiobert_tokenizer = BertTokenizer.from_pretrained(\"saved_biobert_model/tokenizer\")\n\n# Load the model\nbiobert_model = BertForTokenClassification.from_pretrained('dmis-lab/biobert-v1.1', num_labels=3)\n\n# Load the saved state dict\nstate_dict = torch.load(\"saved_biobert_model/pytorch_model.bin\")\nbiobert_model.load_state_dict(state_dict, strict=False)\n\n# Move the model to the device (GPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbiobert_model.to(device)\n\nprint(\"Both models loaded successfully.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:15:57.166204Z","iopub.execute_input":"2024-10-01T17:15:57.166493Z","iopub.status.idle":"2024-10-01T17:16:00.107590Z","shell.execute_reply.started":"2024-10-01T17:15:57.166468Z","shell.execute_reply":"2024-10-01T17:16:00.106690Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/462 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58dffea4aebc4818a3fea61f71eacdef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/433M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"785d67c701414b5784b616c6d15a8fa2"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Both models loaded successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModel, AutoTokenizer\nimport torch.nn as nn\n\n# Load models and tokenizer\nscibert_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\nbiobert_model = AutoModel.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\ntokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n\n# Define the combined classifier class\nclass CombinedClassifier(nn.Module):\n    def __init__(self, scibert_hidden_size, biobert_hidden_size, num_labels):\n        super(CombinedClassifier, self).__init__()\n        self.classifier = nn.Linear(scibert_hidden_size + biobert_hidden_size, num_labels)\n\n    def forward(self, scibert_outputs, biobert_outputs):\n        combined_features = torch.cat((scibert_outputs, biobert_outputs), dim=-1)\n        logits = self.classifier(combined_features)\n        return logits\n\n# Number of labels for the classifier\nnum_labels = 3\nclassifier = CombinedClassifier(scibert_model.config.hidden_size, biobert_model.config.hidden_size, num_labels)\n\n# Input text\ntext = \"The patient was diagnosed with type 2 diabetes mellitus and prescribed metformin\"\nentities = [(27, 54, 'DISEASE'), (70, 79, 'DRUG')]  # Example entities with (start, end, label)\ninputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512, return_offsets_mapping=True)\noffset_mapping = inputs.pop(\"offset_mapping\")\n\ntokens_output = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n\n# Initialize labels with 'O'\ntoken_labels = ['O'] * len(tokens_output)\n\n# Assign 'B' and 'I' labels based on entities\nfor entity_start, entity_end, _ in entities:\n    inside_entity = False\n    for i, (offset_start, offset_end) in enumerate(offset_mapping.squeeze().tolist()):\n        if offset_start == 0 and offset_end == 0:  # Ignore special tokens\n            continue\n        if offset_start >= entity_start and offset_end <= entity_end:\n            if not inside_entity:\n                token_labels[i] = 'B'\n                inside_entity = True\n            else:\n                token_labels[i] = 'I'\n        elif inside_entity:\n            break\n\nprint(\"Tokens       =\", tokens_output)\nprint(\"Final Labels =\", token_labels)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:22:49.499496Z","iopub.execute_input":"2024-10-01T17:22:49.499874Z","iopub.status.idle":"2024-10-01T17:22:51.391332Z","shell.execute_reply.started":"2024-10-01T17:22:49.499846Z","shell.execute_reply":"2024-10-01T17:22:51.390355Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Tokens       = ['[CLS]', 'the', 'patient', 'was', 'diagnosed', 'with', 'type', '2', 'diabetes', 'mellitus', 'and', 'prescribed', 'metformin', '[SEP]']\nFinal Labels = ['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O']\n","output_type":"stream"}]}]}